\chapter{SAMPLE CODE AND EXPLANATION}

\section{MAIN PROGRAM (C++ CODE)}

The main program is written in C++ and is run exclusively on the CPU.  It's responsibilities include setting up the window, initializing the shaders, and initializing and updating the textures and variables passed to the shaders.  After setting up the window, the first major step for the program is to generate the VPL's.  As discussed in section \ref{sec:impdetails} and in the associated figures, the default step up will include the use of 6485 VPL's.  The VPL's will be structured outward in hemispheres from the primary light source in the direction of the primary light source.  Assuming that the direct of the primary light source is pointing downward (negative y-axis), there will be a VPL at every 5 degrees around the y-axis for a total of 72 VPL's (360/5).  Then VPL's will be at every 5 degrees around the the z-axis resulting in 18 VPL's per 90 degree angle.  With this, our hemisphere is almost complete except for the VPL on the y-axis which is not included in the previous calculations. Therefore, we will have 1297 VPL's per hemisphere and 1297 outward rays.  Next, we will chose to have 5 stacked hemispheres flowing outward from the primary light source resulting in 6485 VPL's total.  

The locations of these VPL's will be calculated on start-up based off of the location of the primary light source and the direction it is looking at.  The VPL's will only be updated whenever the primary light source is moved and at no other times reducing overhead.  The VPL's are calculated with an equation similar to below:

\begin{equation}
vpl[x,y,z] = lightPosition[x,y,z] + normal[x,y,z]*(maxDistance - (maxDistance/2^i)) \label{eqn:vplPosition}
\end{equation}

Equation \ref{eqn:vplPosition} calculates the position of the VPL by taking the position of the primary light source and moving in the direction of the primary light source by a particular distance.  This distance is calculated by using the maximum distance allowable (varies based off the dimensions of the scene) and the distances between each VPL on each outward ray is logarithmic which is achieved using $2^i$ where $i$ ranges from 1 to the number of hemispheres or 5 as is default with 1 being the innermost hemisphere and 5 being the outermost hemisphere.  Similarly, we get the VPL direction from the primary light source direction and we get the attenuation from this exponential equation:

\begin{equation}
vplAttenuation = 0.05*pow(2.0,i)\label{eqn:vplNormal}
\end{equation}

Equation \ref{eqn:vplNormal} results in us getting the following attenuation levels for a VPL in each of the 5 hemispheres: $5\%, 10\%, 20\%, 40\%, 80\%$.  

Next, in order for the GPU to have access to the VPL data we have generated above, we store them somehow.  This is done by using 2 1D textures.  The VPL position data and normal data each receive their own texture.  The VPl position data texture is a RGB texture which stores float values of each of the xyz position values.  This is done in C++ with OpenGL by the following statement:


\begin{lstlisting}
glTexImage1D( GL_TEXTURE_1D, 0, GL_RGB, numLights, 0, GL_RGB, 
	GL_FLOAT, &vplDataPos[0]);
\end{lstlisting}

where numLights will be 6485 in this case and vplDataPos is the address for the VPL position data.  Similarly with the normal data we use a 1D texture but this time with RGBA since we will also store the attenuation on top of the xyz values.


\begin{lstlisting}
glTexImage1D( GL_TEXTURE_1D, 0, GL_RGBA, numLights, 0, GL_RGBA, 
	GL_FLOAT, &vplDataNor[0]);
\end{lstlisting}

One problem arises in that the texture will clamp the values stored between 0 and 1.  This is a problem because our VPL data may be negative and will likely be larger than 1 (depending on the dimensions of our scene).  Therefore, prior to storing our data, we must encode our data such that all values will be between 0 and 1 and we can then knowingly decode the data once the texture is in the shaders so our original values are preserved.  This can be done by the following equation:

\begin{equation}
vpl[data] = (vpl[data]/(4*maxDistance))+0.5 \label{eqn:vplEncode}
\end{equation}

Equation \ref{eqn:vplEncode} normalizes the data to be between -0.5 and 0.5 and then adds 0.5 to it to reach the required 0 to 1 range.  The normalization is primarily important to the VPL position data since the VPL normals are already between -1 and 1 and the attenuation is already between 0 and 1. Then once in the shader we can decode the data with the following:

\begin{equation}
vpl[data] = (vpl[data]-0.5)*maxDistance*4.0; \label{eqn:vplDecode}
\end{equation}

Once again, the above steps will only be done at initialization and whenever the primary light source is moved.

The next step is to generate our shadow maps.  As discussed in section \ref{sec:prevwork}, shadow maps are generated by viewing the scene from the perspective of the light source in question.  Then we calculate the distance to the first surface in each pixel to find the depths of the scene.  Using this depth, we can compare the value with the depth of other surfaces and determine whether a point lies in shadow.  In the default set-up, we use 21 shadow maps. One for the primary light source for direct shadows and 20 for the randomly chosen VPL's for the indirect shadows.  In order to do this, we use a single 2D texture array which consists of 21 layers.  This is done in OpenGL by using the following line of code:


\begin{lstlisting}
glTexImage3D(GL_TEXTURE_2D_ARRAY, 0, GL_DEPTH_COMPONENT32, SMWidth, 
SMHeight, numShadowMaps, 0, GL_DEPTH_COMPONENT, GL_FLOAT, NULL);
\end{lstlisting}

The above line of code uses the depth component since we are only interested in the depth value at each pixel and nothing else with shadow maps.  Next, we need the width and height of the shadow map and we declare that we will be storing floats.  Next, in order to capture the depth values of the scene we use OpenGL's frame buffer capabilities.  We render the scene 21 times from the perspective of each light in question to generate the shadow maps and then use these to determine which points lie in shadow from each light's perspective for the final rendering which the user sees.  While generating the shadow maps we are using the fixed function pipeline to perform the rendering from each perspective.  Each time we render the scene, we attach the frame buffer to our shadow map texture using the following:


\begin{lstlisting}
glFramebufferTextureLayer(GL_FRAMEBUFFER, GL_DEPTH_ATTACHMENT, 
	shadowMapTexture, 0, i);
\end{lstlisting}

where $i$ corresponds to which layer of the texture to attach it.  Next, in order for this shadow map to be useful we need a way to transform any given vertex to a coordinate in the shadow map.  We do this by storing the associated modelview and projection matrices used to render the scene each time in a uniform matrix variable that is pass over to the shaders.  This way we will be able to take any vertex, transform it using our transformation matrix and find the correct coordinate in the shadow map to compare it's depth.  This can be done in OpenGL by:


\begin{lstlisting}
glUniformMatrix4fv(lightMatrix, numShadowMaps, GL_FALSE, 
	(GLfloat*)textureMatrix);
\end{lstlisting}

where lightMatrix corresponds to a OpenGL variable that stores the location of the variable, GL\_FALSE tells OpenGL to not transpose the matrix, and textureMatrix is where we are storing our modelview and projection matrices for each shadow map.

These shadow maps will be generated at initialization and for every frame thereafter.  

Next, the main thing left for the main program to perform is to render the final scene with the assistance of the shaders, which will be discussed in the following sections.

\section{VERTEX SHADER CODE}

\section{FRAGMENT SHADER CODE}




